{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " # LightGBM ê¸°ë°˜ 7ì¼ ìˆ˜ìš”ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ (ì œì¶œê¹Œì§€)\n",
        "# -  ë‹¨ì¼ ëª¨ë¸ + horizon(1~7) ë©€í‹°íƒœìŠ¤í¬\n",
        "# - ìº˜ë¦°ë”/ì‹œê³„ì—´ lag & rolling í”¼ì²˜\n",
        "# - ë‹´í•˜/ë¯¸ë¼ì‹œì•„ ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ë°˜ì˜\n",
        "# - 0ì‹¤ì  ì œì™¸ SMAPE\n"
      ],
      "metadata": {
        "id": "dRe6gMUvnQr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MmrmsYzenPwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJf1onSCnAEi"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os, re, glob, random, warnings, math\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "\n",
        "\n",
        "TRAIN_PATH = \"/content/drive/MyDrive/lgaimers7ê¸°/data/train/train.csv\"\n",
        "TEST_DIR   = \"/content/drive/MyDrive/lgaimers7ê¸°/data/test\"\n",
        "SAMPLE_PATH= \"/content/drive/MyDrive/lgaimers7ê¸°/data/sample_submission.csv\"\n",
        "SAVE_PATH  = \"/content/drive/MyDrive/lgaimers7ê¸°/submission/4_0824_submission.csv\"\n",
        "\n",
        "os.makedirs(os.path.dirname(SAVE_PATH), exist_ok=True)\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(TRAIN_PATH)\n",
        "test_files = sorted(glob.glob(os.path.join(TEST_DIR, \"TEST_*.csv\")))\n",
        "sample_submission = pd.read_csv(SAMPLE_PATH)\n",
        "\n",
        "print(f\"ğŸ“Œ Train shape: {train.shape}\")\n",
        "print(f\"ğŸ“Œ Sample shape: {sample_submission.shape}\")\n",
        "print(f\"ğŸ“Œ Test íŒŒì¼ ê°œìˆ˜: {len(test_files)}\")"
      ],
      "metadata": {
        "id": "gELq3WEooN4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ìœ í‹¸í•¨ìˆ˜ & í”¼ì²˜ ì¶”ê°€"
      ],
      "metadata": {
        "id": "Wfs0hPkmoEbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast, unicodedata\n",
        "\n",
        "def normalize_key(s):\n",
        "     if isinstance(s, tuple):\n",
        "        s = s[0]\n",
        "    elif isinstance(s, str) and s.startswith(\"(\") and s.endswith(\")\"):\n",
        "        try:\n",
        "            s = ast.literal_eval(s)[0]\n",
        "        except:\n",
        "            pass\n",
        "    s = str(s)\n",
        "    s = s.replace('\\u3000', ' ')  # ì „ê° ê³µë°± â†’ ë°˜ê°\n",
        "    s = unicodedata.normalize('NFKC', s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "def ensure_store_menu(df):\n",
        "\n",
        "    cols = df.columns\n",
        "    # ë‚ ì§œ ì»¬ëŸ¼ í‘œì¤€í™”\n",
        "    if 'ì˜ì—…ì¼ì' in cols:\n",
        "        df['ì˜ì—…ì¼ì'] = pd.to_datetime(df['ì˜ì—…ì¼ì'], errors='coerce')\n",
        "    # ê²°í•©í‚¤ê°€ ì´ë¯¸ ìˆìœ¼ë©´ ì •ê·œí™”\n",
        "    if 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…' in cols:\n",
        "        df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'] = df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].apply(normalize_key)\n",
        "        parts = df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].str.rsplit('_', n=1, expand=True)\n",
        "        if parts.shape[1] == 2:\n",
        "            df['ì—…ì¥ëª…'] = parts[0]\n",
        "            df['ë©”ë‰´ëª…'] = parts[1]\n",
        "        else:\n",
        "            # ì‹¤íŒ¨ ì‹œ ì „ì²´ë¥¼ ì—…ì¥ëª…ìœ¼ë¡œ, ë©”ë‰´ëª…ì€ ê³µë€\n",
        "            df['ì—…ì¥ëª…'] = df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']\n",
        "            df['ë©”ë‰´ëª…'] = ''\n",
        "    else:\n",
        "        # ë³„ë„ ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ê²°í•©\n",
        "        if {'ì˜ì—…ì¥ëª…','ë©”ë‰´ëª…'}.issubset(cols):\n",
        "            df['ì˜ì—…ì¥ëª…'] = df['ì˜ì—…ì¥ëª…'].apply(normalize_key)\n",
        "            df['ë©”ë‰´ëª…'] = df['ë©”ë‰´ëª…'].apply(normalize_key)\n",
        "            df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'] = (df['ì˜ì—…ì¥ëª…'] + '_' + df['ë©”ë‰´ëª…']).apply(normalize_key)\n",
        "        else:\n",
        "            raise ValueError(\"í•„ìˆ˜ í‚¤ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. (ì˜ì—…ì¥ëª…_ë©”ë‰´ëª… ë˜ëŠ” ì˜ì—…ì¥ëª…/ë©”ë‰´ëª…)\")\n",
        "\n",
        "    # íƒ€ê¹ƒ ì»¬ëŸ¼ ëª… í†µì¼\n",
        "    if 'ë§¤ì¶œìˆ˜ëŸ‰' not in cols:\n",
        "        raise ValueError(\"train/testì— 'ë§¤ì¶œìˆ˜ëŸ‰' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "train = ensure_store_menu(train)\n",
        "\n",
        "\n",
        " def add_calendar_features(df):\n",
        "    df = df.sort_values(['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…','ì˜ì—…ì¼ì']).copy()\n",
        "    df['dow'] = df['ì˜ì—…ì¼ì'].dt.weekday.astype(np.int16)         # 0=ì›”\n",
        "    df['is_weekend'] = df['dow'].isin([5,6]).astype(np.int8)\n",
        "    df['week'] = df['ì˜ì—…ì¼ì'].dt.isocalendar().week.astype(np.int16)\n",
        "    df['month'] = df['ì˜ì—…ì¼ì'].dt.month.astype(np.int8)\n",
        "    df['quarter'] = df['ì˜ì—…ì¼ì'].dt.quarter.astype(np.int8)\n",
        "    df['day'] = df['ì˜ì—…ì¼ì'].dt.day.astype(np.int8)\n",
        "    # ì£¼ê¸° í”¼ì²˜ (ìš”ì¼ 7ì£¼ê¸°)\n",
        "    df['dow_sin'] = np.sin(2*np.pi*df['dow']/7).astype(np.float32)\n",
        "    df['dow_cos'] = np.cos(2*np.pi*df['dow']/7).astype(np.float32)\n",
        "    return df\n",
        "\n",
        "def add_ts_features(df, target='ë§¤ì¶œìˆ˜ëŸ‰', group='ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'):\n",
        "    df = df.sort_values([group,'ì˜ì—…ì¼ì']).copy()\n",
        "\n",
        "    def _grp_feats(g):\n",
        "        g = g.copy()\n",
        "        # lag\n",
        "        for L in [1,7,14]:\n",
        "            g[f'lag_{L}'] = g[target].shift(L)\n",
        "        # rolling mean / std (min_periods=1ì€ ê³¼ì í•© ìœ„í—˜ â†’ ì¶©ë¶„íˆ í™•ë³´ëœ ì‹œì ë§Œ ì‚¬ìš©)\n",
        "        g['rmean_3']  = g[target].shift(1).rolling(3, min_periods=2).mean()\n",
        "        g['rmean_7']  = g[target].shift(1).rolling(7, min_periods=3).mean()\n",
        "        g['rmean_14'] = g[target].shift(1).rolling(14, min_periods=5).mean()\n",
        "        g['rstd_7']   = g[target].shift(1).rolling(7, min_periods=3).std()\n",
        "        # 1ì°¨ ì°¨ë¶„\n",
        "        g['diff_1']   = g[target].diff(1)\n",
        "        return g\n",
        "\n",
        "    df = df.groupby(group, group_keys=False).apply(_grp_feats)\n",
        "    return df\n",
        "\n",
        "train = add_calendar_features(train)\n",
        "train = add_ts_features(train)"
      ],
      "metadata": {
        "id": "UVVml4dRoB03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì¸ì½”ë”© ë° íŠ¹ì • ì—…ì¥ ê°€ì¤‘ì¹˜"
      ],
      "metadata": {
        "id": "vTL67eDroghb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HORIZONS = [1,2,3,4,5,6,7]\n",
        "GROUP_COL = 'ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'\n",
        "TARGET = 'ë§¤ì¶œìˆ˜ëŸ‰'\n",
        "\n",
        " for h in HORIZONS:\n",
        "    train[f'target_h{h}'] = train.groupby(GROUP_COL)[TARGET].shift(-h)\n",
        "\n",
        " base_feats = [\n",
        "    'dow','is_weekend','week','month','quarter','day','dow_sin','dow_cos',\n",
        "    'lag_1','lag_7','lag_14','rmean_3','rmean_7','rmean_14','rstd_7','diff_1'\n",
        "]\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le_store = LabelEncoder()\n",
        "le_menu  = LabelEncoder()\n",
        "\n",
        "train['ì—…ì¥ëª…'] = train['ì—…ì¥ëª…'].astype(str)\n",
        "train['ë©”ë‰´ëª…'] = train['ë©”ë‰´ëª…'].astype(str)\n",
        "\n",
        "le_store.fit(train['ì—…ì¥ëª…'].fillna('NA'))\n",
        "le_menu.fit(train['ë©”ë‰´ëª…'].fillna('NA'))\n",
        "\n",
        "train['ì—…ì¥_code'] = le_store.transform(train['ì—…ì¥ëª…'].fillna('NA')).astype(np.int32)\n",
        "train['ë©”ë‰´_code'] = le_menu.transform(train['ë©”ë‰´ëª…'].fillna('NA')).astype(np.int32)\n",
        "\n",
        "feat_cols = base_feats + ['ì—…ì¥_code','ë©”ë‰´_code']\n",
        "\n",
        "stack_list = []\n",
        "keep_cols = ['ì˜ì—…ì¼ì'] + feat_cols + ['ì—…ì¥ëª…','ë©”ë‰´ëª…', GROUP_COL]\n",
        "\n",
        "for h in HORIZONS:\n",
        "    tmp = train.dropna(subset=[f'target_h{h}']).copy()\n",
        "    tmp['h'] = h\n",
        "    tmp['target'] = tmp[f'target_h{h}']\n",
        "    stack_list.append(tmp[keep_cols + ['h','target']])\n",
        "\n",
        "train_stack = pd.concat(stack_list, axis=0, ignore_index=True)\n",
        "train_stack = train_stack.dropna(subset=feat_cols)  # lag/roll ê²°ì¸¡ ì œê±°\n",
        "\n",
        " def norm_name(s):\n",
        "    return normalize_key(str(s))\n",
        "\n",
        "FOCUS_STORES = ['ë‹´í•˜', 'ë¯¸ë¼ì‹œì•„']\n",
        "FOCUS_STORES_NORM = [normalize_key(s) for s in FOCUS_STORES]\n",
        "\n",
        "def make_weight(row):\n",
        "    w = 1.0\n",
        "    if any(fs in norm_name(row['ì—…ì¥ëª…']) for fs in FOCUS_STORES_NORM):\n",
        "        w *= 2.0\n",
        "    if row['target'] == 0:\n",
        "        w *= 0.5\n",
        "    return w\n",
        "\n",
        "train_stack['weight'] = train_stack.apply(make_weight, axis=1).astype(np.float32)\n",
        "\n",
        "train_stack['ì˜ì—…ì¼ì'] = pd.to_datetime(train_stack['ì˜ì—…ì¼ì'], errors='coerce')\n",
        "cutoff_date = train_stack['ì˜ì—…ì¼ì'].dropna().quantile(0.9)\n",
        "train_stack['is_val'] = train_stack['ì˜ì—…ì¼ì'] >= cutoff_date\n",
        "\n",
        "X = train_stack[feat_cols + ['h']].astype(np.float32)\n",
        "y = train_stack['target'].astype(np.float32)\n",
        "w = train_stack['weight'].astype(np.float32).values\n",
        "\n",
        "X_train = X[~train_stack['is_val']]; y_train = y[~train_stack['is_val']]; w_train = w[~train_stack['is_val']]\n",
        "X_valid = X[ train_stack['is_val']]; y_valid = y[ train_stack['is_val']]\n",
        "print(\"ğŸ§± Train/Valid shapes:\", X_train.shape, X_valid.shape)\n",
        "print(\"ğŸ“… cutoff_date:\", cutoff_date)"
      ],
      "metadata": {
        "id": "MsLNSRXaokbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lgbm + blending"
      ],
      "metadata": {
        "id": "HEzqBkBuovjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def smape_ignore_zero(y_true, y_pred, eps=1e-9):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    mask = y_true != 0\n",
        "    if mask.sum() == 0:\n",
        "        return np.nan\n",
        "    yt = y_true[mask]; yp = y_pred[mask]\n",
        "    return np.mean(2.0 * np.abs(yp - yt) / (np.abs(yt) + np.abs(yp) + eps))\n",
        "\n",
        "\n",
        "lgbm = lgb.LGBMRegressor(\n",
        "    n_estimators=5000, learning_rate=0.03,\n",
        "    num_leaves=64, max_depth=-1,\n",
        "    subsample=0.9, colsample_bytree=0.8,\n",
        "    min_child_samples=40, reg_alpha=1.0, reg_lambda=2.0,\n",
        "    n_jobs=-1, random_state=42\n",
        ")\n",
        "lgbm.fit(\n",
        "    X_train, y_train,\n",
        "    sample_weight=w_train,\n",
        "    eval_set=[(X_valid, y_valid)],\n",
        "    eval_metric='l1',\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]\n",
        ")\n",
        "valid_pred = lgbm.predict(X_valid, num_iteration=lgbm.best_iteration_)\n",
        "valid_pred = np.clip(valid_pred, 0, None)\n",
        "print(f\"ğŸ” LGBM valid SMAPE(0ì œì™¸): {smape_ignore_zero(y_valid.values, valid_pred):.6f}\")\n",
        "\n",
        "train_stack['nonzero'] = (train_stack['target'] > 0).astype(np.int8)\n",
        "X_all = train_stack[feat_cols + ['h']].astype(np.float32)\n",
        "\n",
        "clf = lgb.LGBMClassifier(\n",
        "    n_estimators=3000, learning_rate=0.03,\n",
        "    num_leaves=63, subsample=0.9, colsample_bytree=0.8,\n",
        "    min_child_samples=40, reg_alpha=0.5, reg_lambda=1.0, random_state=42\n",
        ")\n",
        "clf.fit(\n",
        "    X_all[~train_stack['is_val']], train_stack['nonzero'][~train_stack['is_val']],\n",
        "    sample_weight=train_stack.loc[~train_stack['is_val'], 'weight'],\n",
        "    eval_set=[(X_all[train_stack['is_val']], train_stack['nonzero'][train_stack['is_val']])],\n",
        "    callbacks=[lgb.early_stopping(200, verbose=False)]\n",
        ")\n",
        "\n",
        "reg = lgb.LGBMRegressor(\n",
        "    n_estimators=5000, learning_rate=0.03,\n",
        "    num_leaves=127, subsample=0.9, colsample_bytree=0.8,\n",
        "    min_child_samples=30, reg_alpha=1.0, reg_lambda=2.0,\n",
        "    objective='tweedie', tweedie_variance_power=1.2, random_state=42\n",
        ")\n",
        "nz = train_stack['nonzero'] == 1\n",
        "y_reg = np.log1p(train_stack.loc[nz, 'target'])\n",
        "reg.fit(\n",
        "    X_all[nz & (~train_stack['is_val'])], y_reg[nz & (~train_stack['is_val'])],\n",
        "    sample_weight=train_stack.loc[nz & (~train_stack['is_val']), 'weight'],\n",
        "    eval_set=[(X_all[nz & (train_stack['is_val'])], y_reg[nz & (train_stack['is_val'])])],\n",
        "    eval_metric='l1', callbacks=[lgb.early_stopping(300, verbose=False)]\n",
        ")\n",
        "\n",
        "p_val  = clf.predict_proba(X_valid)[:, 1]\n",
        "mu_val = np.expm1(reg.predict(X_valid))\n",
        "zi_val = np.clip(p_val * mu_val, 0, None)\n",
        "\n",
        "#ë„ìš°ë‚˜ì´ë¸Œ\n",
        "train_nonval = train[train['ì˜ì—…ì¼ì'] < cutoff_date].copy()\n",
        "train_nonval['dow'] = train_nonval['ì˜ì—…ì¼ì'].dt.weekday\n",
        "grp_dow = train_nonval.groupby(['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…','dow'])['ë§¤ì¶œìˆ˜ëŸ‰'].mean()\n",
        "\n",
        "\n",
        "valid_meta = train_stack.loc[train_stack['is_val'], [GROUP_COL, 'ì˜ì—…ì¼ì']].copy()\n",
        "valid_meta['dow'] = valid_meta['ì˜ì—…ì¼ì'].dt.weekday\n",
        "naive_val = valid_meta.set_index([GROUP_COL, 'dow']).index.map(grp_dow).to_series(index=valid_meta.index).values\n",
        "naive_val = np.where(np.isnan(naive_val), valid_pred.mean(), naive_val)\n",
        "\n",
        "\n",
        "\n",
        "def optimize_blend_weights(y_true, preds, step=0.02):\n",
        "    lgbm_v, zi_v, nv = preds\n",
        "    grid = np.arange(0.0, 1.0+1e-9, step)\n",
        "    best_w, best = (1.0,0.0,0.0), smape_ignore_zero(y_true, lgbm_v)\n",
        "    for w1 in grid:\n",
        "        for w2 in grid:\n",
        "            w3 = 1.0 - w1 - w2\n",
        "            if w3 < 0: continue\n",
        "            blend = w1*lgbm_v + w2*zi_v + w3*nv\n",
        "            score = smape_ignore_zero(y_true, np.clip(blend, 0, None))\n",
        "            if not np.isnan(score) and score < best:\n",
        "                best, best_w = score, (w1,w2,w3)\n",
        "    return best_w, best\n",
        "\n",
        "best_w, best_score = optimize_blend_weights(y_valid.values, (valid_pred, zi_val, naive_val), step=0.02)\n",
        "w1, w2, w3 = best_w\n",
        "print(f\"ğŸ Best weights: LGBM={w1:.2f} ZI={w2:.2f} Naive={w3:.2f} | SMAPE={best_score:.6f}\")\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# í¬ì»¤ìŠ¤ ì—…ì¥ ìŠ¤ì¼€ì¼ Î± íƒìƒ‰ (ë‹´í•˜/ë¯¸ë¼ì‹œì•„ë§Œ)\n",
        "# ------------------------------------------\n",
        "valid_store = train_stack.loc[train_stack['is_val'], 'ì—…ì¥ëª…'].map(norm_name).values\n",
        "focus_mask = np.array([any(fs in st for fs in FOCUS_STORES_NORM) for st in valid_store])\n",
        "\n",
        "\n",
        "base_blend = np.clip(w1*valid_pred + w2*zi_val + w3*naive_val, 0, None)\n",
        "\n",
        "def search_focus_alpha(y_true, blend, mask, alphas=np.round(np.arange(0.70, 1.31, 0.02),2)):\n",
        "    best_a, best = 1.00, smape_ignore_zero(y_true, blend)\n",
        "    for a in alphas:\n",
        "        adj = blend.copy()\n",
        "        adj[mask] = np.clip(a*adj[mask], 0, None)\n",
        "        score = smape_ignore_zero(y_true, adj)\n",
        "        if score < best: best, best_a = score, a\n",
        "    return best_a, best\n",
        "\n",
        "alpha, best_blend = search_focus_alpha(y_valid.values, base_blend, focus_mask)\n",
        "print(f\"ğŸ¯ Best focus alpha: {alpha:.2f} | SMAPE={best_blend:.6f}\")\n",
        "\n",
        "# ìµœì¢… ê²€ì¦ ë¸”ë Œë“œ ì ìˆ˜\n",
        "final_valid = base_blend.copy()\n",
        "final_valid[focus_mask] = np.clip(alpha*final_valid[focus_mask], 0, None)\n",
        "print(f\"ğŸŒ€ Final blended valid SMAPE(0ì œì™¸): {smape_ignore_zero(y_valid.values, final_valid):.6f}\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ì—ì„œ ì“¸ ì „ì—­ íŒŒë¼ë¯¸í„° ì €ì¥\n",
        "BLEND_WEIGHTS = (w1, w2, w3)\n",
        "FOCUS_ALPHA = alpha"
      ],
      "metadata": {
        "id": "38YC84HmnLLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##test"
      ],
      "metadata": {
        "id": "ROUBMjyypNxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_test_features(test_df):\n",
        "    test_df = ensure_store_menu(test_df)\n",
        "    test_df = add_calendar_features(test_df)\n",
        "    test_df = add_ts_features(test_df)\n",
        "\n",
        "    def safe_transform(le, series):\n",
        "        mapping = {k: i for i, k in enumerate(le.classes_)}\n",
        "        code = series.map(mapping).astype('Int64')  # pandas nullable int\n",
        "        unk_code = (pd.Series(mapping.values()).max() if len(mapping) else -1) + 1\n",
        "        return code.fillna(unk_code).astype(np.int32)\n",
        "\n",
        "    test_df['ì—…ì¥ëª…'] = test_df['ì—…ì¥ëª…'].astype(str)\n",
        "    test_df['ë©”ë‰´ëª…'] = test_df['ë©”ë‰´ëª…'].astype(str)\n",
        "    test_df['ì—…ì¥_code'] = safe_transform(le_store, test_df['ì—…ì¥ëª…'])\n",
        "    test_df['ë©”ë‰´_code'] = safe_transform(le_menu,  test_df['ë©”ë‰´ëª…'])\n",
        "    return test_df\n",
        "\n",
        "# (ë©”ë‰´,ìš”ì¼) í‰ê· ì€ trainì—ì„œë§Œ ê³„ì‚° â†’ ìœ„ grp ì¬ì‚¬ìš©\n",
        "def predict_test_file(path):\n",
        "    filename = os.path.basename(path)\n",
        "    prefix = re.search(r'(TEST_\\d+)', filename).group(1)\n",
        "\n",
        "    test_df = pd.read_csv(path)\n",
        "    test_df = make_test_features(test_df)\n",
        "\n",
        "    # predict_test_file ë‚´ë¶€, feats_last ë§Œë“  ì§í›„\n",
        "    last_date = test_df.groupby('ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…')['ì˜ì—…ì¼ì'].max()\n",
        "    last_dow  = last_date.dt.weekday  # 0~6\n",
        "\n",
        "    # ê·¸ë£¹ë³„ ë§ˆì§€ë§‰ ì‹œì  í”¼ì²˜\n",
        "    test_df = test_df.sort_values(['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…','ì˜ì—…ì¼ì'])\n",
        "    last_idx = test_df.groupby('ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…')['ì˜ì—…ì¼ì'].idxmax()\n",
        "    feats_last = test_df.loc[last_idx, ['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…','ì—…ì¥ëª…','ë©”ë‰´ëª…'] + feat_cols].copy()\n",
        "    feats_last = feats_last.dropna(subset=feat_cols)\n",
        "    if len(feats_last) == 0:\n",
        "        return pd.DataFrame(columns=['ì˜ì—…ì¼ì','ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…','ë§¤ì¶œìˆ˜ëŸ‰'])\n",
        "\n",
        "    # ê³µí†µ X\n",
        "    def infer_one_h(h):\n",
        "        tmp = feats_last.copy()\n",
        "        tmp['h'] = h\n",
        "        Xh = tmp[feat_cols + ['h']].astype(np.float32)\n",
        "        # LGBM\n",
        "        lgbh = np.clip(lgbm.predict(Xh, num_iteration=lgbm.best_iteration_), 0, None)\n",
        "        # ZI\n",
        "        ph  = clf.predict_proba(Xh)[:,1]\n",
        "        muh = np.expm1(reg.predict(Xh))\n",
        "        zih = np.clip(ph * muh, 0, None)\n",
        "\n",
        "        # ---- DOW naive (horizonë³„ ìš”ì¼ ë°˜ì˜) ----\n",
        "        keys = tmp['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].values\n",
        "        dow_h = (last_dow.loc[keys].values + h) % 7\n",
        "\n",
        "        idx = pd.MultiIndex.from_arrays([keys, dow_h])\n",
        "        naive_vals = grp_dow.reindex(idx).to_numpy()\n",
        "\n",
        "        if np.any(pd.isna(naive_vals)):\n",
        "            menu_mean = train.groupby('ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…')['ë§¤ì¶œìˆ˜ëŸ‰'].mean()\n",
        "            fill1 = pd.Series(keys).map(menu_mean).to_numpy()\n",
        "            fill1 = np.where(pd.isna(fill1), float(menu_mean.mean()), fill1)\n",
        "            naive_vals = np.where(pd.isna(naive_vals), fill1, naive_vals)\n",
        "\n",
        "        # ---- ë¸”ë Œë“œ (ê²€ì¦ì—ì„œ í•™ìŠµëœ ì „ì—­ ê°€ì¤‘ì¹˜ + í¬ì»¤ìŠ¤ ì—…ì¥ Î±) ----\n",
        "        w1, w2, w3 = BLEND_WEIGHTS\n",
        "        blend = w1*lgbh + w2*zih + w3*naive_vals\n",
        "\n",
        "        stores = tmp['ì—…ì¥ëª…'].map(norm_name).values  # â† tmpë¡œ í†µì¼\n",
        "        focus_mask = np.array([any(fs in st for fs in FOCUS_STORES_NORM) for st in stores])\n",
        "\n",
        "        blend[focus_mask] = FOCUS_ALPHA * blend[focus_mask]\n",
        "        blend = np.clip(blend, 0, None)\n",
        "\n",
        "\n",
        "        out = tmp[['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…']].copy()\n",
        "        out['ì˜ì—…ì¼ì'] = f\"{prefix}+{h}ì¼\"\n",
        "        out['ë§¤ì¶œìˆ˜ëŸ‰'] = blend\n",
        "        return out\n",
        "\n",
        "    outs = [infer_one_h(h) for h in HORIZONS]\n",
        "    return pd.concat(outs, ignore_index=True)\n",
        "\n",
        "all_preds = []\n",
        "for p in test_files:\n",
        "    all_preds.append(predict_test_file(p))\n",
        "pred_full = pd.concat(all_preds, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "O5zxCXyepHXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##submission"
      ],
      "metadata": {
        "id": "cjPuirBnpKtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# ì œì¶œ ë³€í™˜\n",
        "# -----------------------------\n",
        "def convert_to_submission(pred_df, sample_df):\n",
        "    out = sample_df.copy()\n",
        "    pred_df = pred_df.copy()\n",
        "    pred_df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'] = pred_df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].apply(normalize_key)\n",
        "    pred_df['ì˜ì—…ì¼ì'] = pred_df['ì˜ì—…ì¼ì'].astype(str)\n",
        "\n",
        "    # ìƒ˜í”Œ ì»¬ëŸ¼ ì •ê·œí™”\n",
        "    col_map = {}\n",
        "    for c in out.columns:\n",
        "        if c == 'ì˜ì—…ì¼ì':\n",
        "            continue\n",
        "        col_map[c] = normalize_key(c)\n",
        "\n",
        "    # key â†’ value\n",
        "    pred_df['norm_menu'] = pred_df['ì˜ì—…ì¥ëª…_ë©”ë‰´ëª…'].apply(normalize_key)\n",
        "    key = list(zip(pred_df['ì˜ì—…ì¼ì'], pred_df['norm_menu']))\n",
        "    pred_dict = dict(zip(key, pred_df['ë§¤ì¶œìˆ˜ëŸ‰']))\n",
        "\n",
        "    # ì±„ìš°ê¸°\n",
        "    for i in range(len(out)):\n",
        "        date = str(out.loc[i, 'ì˜ì—…ì¼ì'])\n",
        "        for c in out.columns[1:]:\n",
        "            out.loc[i, c] = pred_dict.get((date, col_map[c]), 0)\n",
        "    return out\n",
        "\n",
        "submission = convert_to_submission(pred_full, sample_submission)\n",
        "submission.to_csv(SAVE_PATH, index=False, encoding=\"utf-8-sig\")\n",
        "print(\"ìµœì¢… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ:\", SAVE_PATH)\n",
        "print(submission.head(3).to_string()[:1000])"
      ],
      "metadata": {
        "id": "Z4t5dMT6pKF-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}