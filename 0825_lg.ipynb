{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " # LightGBM 기반 7일 수요예측 파이프라인 (제출까지)\n",
        "# -  단일 모델 + horizon(1~7) 멀티태스크\n",
        "# - 캘린더/시계열 lag & rolling 피처\n",
        "# - 담하/미라시아 샘플 가중치 반영\n",
        "# - 0실적 제외 SMAPE\n"
      ],
      "metadata": {
        "id": "dRe6gMUvnQr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MmrmsYzenPwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJf1onSCnAEi"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os, re, glob, random, warnings, math\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "\n",
        "\n",
        "TRAIN_PATH = \"/content/drive/MyDrive/lgaimers7기/data/train/train.csv\"\n",
        "TEST_DIR   = \"/content/drive/MyDrive/lgaimers7기/data/test\"\n",
        "SAMPLE_PATH= \"/content/drive/MyDrive/lgaimers7기/data/sample_submission.csv\"\n",
        "SAVE_PATH  = \"/content/drive/MyDrive/lgaimers7기/submission/4_0824_submission.csv\"\n",
        "\n",
        "os.makedirs(os.path.dirname(SAVE_PATH), exist_ok=True)\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(TRAIN_PATH)\n",
        "test_files = sorted(glob.glob(os.path.join(TEST_DIR, \"TEST_*.csv\")))\n",
        "sample_submission = pd.read_csv(SAMPLE_PATH)\n",
        "\n",
        "print(f\"📌 Train shape: {train.shape}\")\n",
        "print(f\"📌 Sample shape: {sample_submission.shape}\")\n",
        "print(f\"📌 Test 파일 개수: {len(test_files)}\")"
      ],
      "metadata": {
        "id": "gELq3WEooN4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "유틸함수 & 피처 추가"
      ],
      "metadata": {
        "id": "Wfs0hPkmoEbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast, unicodedata\n",
        "\n",
        "def normalize_key(s):\n",
        "     if isinstance(s, tuple):\n",
        "        s = s[0]\n",
        "    elif isinstance(s, str) and s.startswith(\"(\") and s.endswith(\")\"):\n",
        "        try:\n",
        "            s = ast.literal_eval(s)[0]\n",
        "        except:\n",
        "            pass\n",
        "    s = str(s)\n",
        "    s = s.replace('\\u3000', ' ')  # 전각 공백 → 반각\n",
        "    s = unicodedata.normalize('NFKC', s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "def ensure_store_menu(df):\n",
        "\n",
        "    cols = df.columns\n",
        "    # 날짜 컬럼 표준화\n",
        "    if '영업일자' in cols:\n",
        "        df['영업일자'] = pd.to_datetime(df['영업일자'], errors='coerce')\n",
        "    # 결합키가 이미 있으면 정규화\n",
        "    if '영업장명_메뉴명' in cols:\n",
        "        df['영업장명_메뉴명'] = df['영업장명_메뉴명'].apply(normalize_key)\n",
        "        parts = df['영업장명_메뉴명'].str.rsplit('_', n=1, expand=True)\n",
        "        if parts.shape[1] == 2:\n",
        "            df['업장명'] = parts[0]\n",
        "            df['메뉴명'] = parts[1]\n",
        "        else:\n",
        "            # 실패 시 전체를 업장명으로, 메뉴명은 공란\n",
        "            df['업장명'] = df['영업장명_메뉴명']\n",
        "            df['메뉴명'] = ''\n",
        "    else:\n",
        "        # 별도 컬럼이 있다면 결합\n",
        "        if {'영업장명','메뉴명'}.issubset(cols):\n",
        "            df['영업장명'] = df['영업장명'].apply(normalize_key)\n",
        "            df['메뉴명'] = df['메뉴명'].apply(normalize_key)\n",
        "            df['영업장명_메뉴명'] = (df['영업장명'] + '_' + df['메뉴명']).apply(normalize_key)\n",
        "        else:\n",
        "            raise ValueError(\"필수 키 컬럼이 없습니다. (영업장명_메뉴명 또는 영업장명/메뉴명)\")\n",
        "\n",
        "    # 타깃 컬럼 명 통일\n",
        "    if '매출수량' not in cols:\n",
        "        raise ValueError(\"train/test에 '매출수량' 컬럼이 필요합니다.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "train = ensure_store_menu(train)\n",
        "\n",
        "\n",
        " def add_calendar_features(df):\n",
        "    df = df.sort_values(['영업장명_메뉴명','영업일자']).copy()\n",
        "    df['dow'] = df['영업일자'].dt.weekday.astype(np.int16)         # 0=월\n",
        "    df['is_weekend'] = df['dow'].isin([5,6]).astype(np.int8)\n",
        "    df['week'] = df['영업일자'].dt.isocalendar().week.astype(np.int16)\n",
        "    df['month'] = df['영업일자'].dt.month.astype(np.int8)\n",
        "    df['quarter'] = df['영업일자'].dt.quarter.astype(np.int8)\n",
        "    df['day'] = df['영업일자'].dt.day.astype(np.int8)\n",
        "    # 주기 피처 (요일 7주기)\n",
        "    df['dow_sin'] = np.sin(2*np.pi*df['dow']/7).astype(np.float32)\n",
        "    df['dow_cos'] = np.cos(2*np.pi*df['dow']/7).astype(np.float32)\n",
        "    return df\n",
        "\n",
        "def add_ts_features(df, target='매출수량', group='영업장명_메뉴명'):\n",
        "    df = df.sort_values([group,'영업일자']).copy()\n",
        "\n",
        "    def _grp_feats(g):\n",
        "        g = g.copy()\n",
        "        # lag\n",
        "        for L in [1,7,14]:\n",
        "            g[f'lag_{L}'] = g[target].shift(L)\n",
        "        # rolling mean / std (min_periods=1은 과적합 위험 → 충분히 확보된 시점만 사용)\n",
        "        g['rmean_3']  = g[target].shift(1).rolling(3, min_periods=2).mean()\n",
        "        g['rmean_7']  = g[target].shift(1).rolling(7, min_periods=3).mean()\n",
        "        g['rmean_14'] = g[target].shift(1).rolling(14, min_periods=5).mean()\n",
        "        g['rstd_7']   = g[target].shift(1).rolling(7, min_periods=3).std()\n",
        "        # 1차 차분\n",
        "        g['diff_1']   = g[target].diff(1)\n",
        "        return g\n",
        "\n",
        "    df = df.groupby(group, group_keys=False).apply(_grp_feats)\n",
        "    return df\n",
        "\n",
        "train = add_calendar_features(train)\n",
        "train = add_ts_features(train)"
      ],
      "metadata": {
        "id": "UVVml4dRoB03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "인코딩 및 특정 업장 가중치"
      ],
      "metadata": {
        "id": "vTL67eDroghb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HORIZONS = [1,2,3,4,5,6,7]\n",
        "GROUP_COL = '영업장명_메뉴명'\n",
        "TARGET = '매출수량'\n",
        "\n",
        " for h in HORIZONS:\n",
        "    train[f'target_h{h}'] = train.groupby(GROUP_COL)[TARGET].shift(-h)\n",
        "\n",
        " base_feats = [\n",
        "    'dow','is_weekend','week','month','quarter','day','dow_sin','dow_cos',\n",
        "    'lag_1','lag_7','lag_14','rmean_3','rmean_7','rmean_14','rstd_7','diff_1'\n",
        "]\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le_store = LabelEncoder()\n",
        "le_menu  = LabelEncoder()\n",
        "\n",
        "train['업장명'] = train['업장명'].astype(str)\n",
        "train['메뉴명'] = train['메뉴명'].astype(str)\n",
        "\n",
        "le_store.fit(train['업장명'].fillna('NA'))\n",
        "le_menu.fit(train['메뉴명'].fillna('NA'))\n",
        "\n",
        "train['업장_code'] = le_store.transform(train['업장명'].fillna('NA')).astype(np.int32)\n",
        "train['메뉴_code'] = le_menu.transform(train['메뉴명'].fillna('NA')).astype(np.int32)\n",
        "\n",
        "feat_cols = base_feats + ['업장_code','메뉴_code']\n",
        "\n",
        "stack_list = []\n",
        "keep_cols = ['영업일자'] + feat_cols + ['업장명','메뉴명', GROUP_COL]\n",
        "\n",
        "for h in HORIZONS:\n",
        "    tmp = train.dropna(subset=[f'target_h{h}']).copy()\n",
        "    tmp['h'] = h\n",
        "    tmp['target'] = tmp[f'target_h{h}']\n",
        "    stack_list.append(tmp[keep_cols + ['h','target']])\n",
        "\n",
        "train_stack = pd.concat(stack_list, axis=0, ignore_index=True)\n",
        "train_stack = train_stack.dropna(subset=feat_cols)  # lag/roll 결측 제거\n",
        "\n",
        " def norm_name(s):\n",
        "    return normalize_key(str(s))\n",
        "\n",
        "FOCUS_STORES = ['담하', '미라시아']\n",
        "FOCUS_STORES_NORM = [normalize_key(s) for s in FOCUS_STORES]\n",
        "\n",
        "def make_weight(row):\n",
        "    w = 1.0\n",
        "    if any(fs in norm_name(row['업장명']) for fs in FOCUS_STORES_NORM):\n",
        "        w *= 2.0\n",
        "    if row['target'] == 0:\n",
        "        w *= 0.5\n",
        "    return w\n",
        "\n",
        "train_stack['weight'] = train_stack.apply(make_weight, axis=1).astype(np.float32)\n",
        "\n",
        "train_stack['영업일자'] = pd.to_datetime(train_stack['영업일자'], errors='coerce')\n",
        "cutoff_date = train_stack['영업일자'].dropna().quantile(0.9)\n",
        "train_stack['is_val'] = train_stack['영업일자'] >= cutoff_date\n",
        "\n",
        "X = train_stack[feat_cols + ['h']].astype(np.float32)\n",
        "y = train_stack['target'].astype(np.float32)\n",
        "w = train_stack['weight'].astype(np.float32).values\n",
        "\n",
        "X_train = X[~train_stack['is_val']]; y_train = y[~train_stack['is_val']]; w_train = w[~train_stack['is_val']]\n",
        "X_valid = X[ train_stack['is_val']]; y_valid = y[ train_stack['is_val']]\n",
        "print(\"🧱 Train/Valid shapes:\", X_train.shape, X_valid.shape)\n",
        "print(\"📅 cutoff_date:\", cutoff_date)"
      ],
      "metadata": {
        "id": "MsLNSRXaokbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lgbm + blending"
      ],
      "metadata": {
        "id": "HEzqBkBuovjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def smape_ignore_zero(y_true, y_pred, eps=1e-9):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    mask = y_true != 0\n",
        "    if mask.sum() == 0:\n",
        "        return np.nan\n",
        "    yt = y_true[mask]; yp = y_pred[mask]\n",
        "    return np.mean(2.0 * np.abs(yp - yt) / (np.abs(yt) + np.abs(yp) + eps))\n",
        "\n",
        "\n",
        "lgbm = lgb.LGBMRegressor(\n",
        "    n_estimators=5000, learning_rate=0.03,\n",
        "    num_leaves=64, max_depth=-1,\n",
        "    subsample=0.9, colsample_bytree=0.8,\n",
        "    min_child_samples=40, reg_alpha=1.0, reg_lambda=2.0,\n",
        "    n_jobs=-1, random_state=42\n",
        ")\n",
        "lgbm.fit(\n",
        "    X_train, y_train,\n",
        "    sample_weight=w_train,\n",
        "    eval_set=[(X_valid, y_valid)],\n",
        "    eval_metric='l1',\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]\n",
        ")\n",
        "valid_pred = lgbm.predict(X_valid, num_iteration=lgbm.best_iteration_)\n",
        "valid_pred = np.clip(valid_pred, 0, None)\n",
        "print(f\"🔎 LGBM valid SMAPE(0제외): {smape_ignore_zero(y_valid.values, valid_pred):.6f}\")\n",
        "\n",
        "train_stack['nonzero'] = (train_stack['target'] > 0).astype(np.int8)\n",
        "X_all = train_stack[feat_cols + ['h']].astype(np.float32)\n",
        "\n",
        "clf = lgb.LGBMClassifier(\n",
        "    n_estimators=3000, learning_rate=0.03,\n",
        "    num_leaves=63, subsample=0.9, colsample_bytree=0.8,\n",
        "    min_child_samples=40, reg_alpha=0.5, reg_lambda=1.0, random_state=42\n",
        ")\n",
        "clf.fit(\n",
        "    X_all[~train_stack['is_val']], train_stack['nonzero'][~train_stack['is_val']],\n",
        "    sample_weight=train_stack.loc[~train_stack['is_val'], 'weight'],\n",
        "    eval_set=[(X_all[train_stack['is_val']], train_stack['nonzero'][train_stack['is_val']])],\n",
        "    callbacks=[lgb.early_stopping(200, verbose=False)]\n",
        ")\n",
        "\n",
        "reg = lgb.LGBMRegressor(\n",
        "    n_estimators=5000, learning_rate=0.03,\n",
        "    num_leaves=127, subsample=0.9, colsample_bytree=0.8,\n",
        "    min_child_samples=30, reg_alpha=1.0, reg_lambda=2.0,\n",
        "    objective='tweedie', tweedie_variance_power=1.2, random_state=42\n",
        ")\n",
        "nz = train_stack['nonzero'] == 1\n",
        "y_reg = np.log1p(train_stack.loc[nz, 'target'])\n",
        "reg.fit(\n",
        "    X_all[nz & (~train_stack['is_val'])], y_reg[nz & (~train_stack['is_val'])],\n",
        "    sample_weight=train_stack.loc[nz & (~train_stack['is_val']), 'weight'],\n",
        "    eval_set=[(X_all[nz & (train_stack['is_val'])], y_reg[nz & (train_stack['is_val'])])],\n",
        "    eval_metric='l1', callbacks=[lgb.early_stopping(300, verbose=False)]\n",
        ")\n",
        "\n",
        "p_val  = clf.predict_proba(X_valid)[:, 1]\n",
        "mu_val = np.expm1(reg.predict(X_valid))\n",
        "zi_val = np.clip(p_val * mu_val, 0, None)\n",
        "\n",
        "#도우나이브\n",
        "train_nonval = train[train['영업일자'] < cutoff_date].copy()\n",
        "train_nonval['dow'] = train_nonval['영업일자'].dt.weekday\n",
        "grp_dow = train_nonval.groupby(['영업장명_메뉴명','dow'])['매출수량'].mean()\n",
        "\n",
        "\n",
        "valid_meta = train_stack.loc[train_stack['is_val'], [GROUP_COL, '영업일자']].copy()\n",
        "valid_meta['dow'] = valid_meta['영업일자'].dt.weekday\n",
        "naive_val = valid_meta.set_index([GROUP_COL, 'dow']).index.map(grp_dow).to_series(index=valid_meta.index).values\n",
        "naive_val = np.where(np.isnan(naive_val), valid_pred.mean(), naive_val)\n",
        "\n",
        "\n",
        "\n",
        "def optimize_blend_weights(y_true, preds, step=0.02):\n",
        "    lgbm_v, zi_v, nv = preds\n",
        "    grid = np.arange(0.0, 1.0+1e-9, step)\n",
        "    best_w, best = (1.0,0.0,0.0), smape_ignore_zero(y_true, lgbm_v)\n",
        "    for w1 in grid:\n",
        "        for w2 in grid:\n",
        "            w3 = 1.0 - w1 - w2\n",
        "            if w3 < 0: continue\n",
        "            blend = w1*lgbm_v + w2*zi_v + w3*nv\n",
        "            score = smape_ignore_zero(y_true, np.clip(blend, 0, None))\n",
        "            if not np.isnan(score) and score < best:\n",
        "                best, best_w = score, (w1,w2,w3)\n",
        "    return best_w, best\n",
        "\n",
        "best_w, best_score = optimize_blend_weights(y_valid.values, (valid_pred, zi_val, naive_val), step=0.02)\n",
        "w1, w2, w3 = best_w\n",
        "print(f\"🏁 Best weights: LGBM={w1:.2f} ZI={w2:.2f} Naive={w3:.2f} | SMAPE={best_score:.6f}\")\n",
        "\n",
        "\n",
        "# ------------------------------------------\n",
        "# 포커스 업장 스케일 α 탐색 (담하/미라시아만)\n",
        "# ------------------------------------------\n",
        "valid_store = train_stack.loc[train_stack['is_val'], '업장명'].map(norm_name).values\n",
        "focus_mask = np.array([any(fs in st for fs in FOCUS_STORES_NORM) for st in valid_store])\n",
        "\n",
        "\n",
        "base_blend = np.clip(w1*valid_pred + w2*zi_val + w3*naive_val, 0, None)\n",
        "\n",
        "def search_focus_alpha(y_true, blend, mask, alphas=np.round(np.arange(0.70, 1.31, 0.02),2)):\n",
        "    best_a, best = 1.00, smape_ignore_zero(y_true, blend)\n",
        "    for a in alphas:\n",
        "        adj = blend.copy()\n",
        "        adj[mask] = np.clip(a*adj[mask], 0, None)\n",
        "        score = smape_ignore_zero(y_true, adj)\n",
        "        if score < best: best, best_a = score, a\n",
        "    return best_a, best\n",
        "\n",
        "alpha, best_blend = search_focus_alpha(y_valid.values, base_blend, focus_mask)\n",
        "print(f\"🎯 Best focus alpha: {alpha:.2f} | SMAPE={best_blend:.6f}\")\n",
        "\n",
        "# 최종 검증 블렌드 점수\n",
        "final_valid = base_blend.copy()\n",
        "final_valid[focus_mask] = np.clip(alpha*final_valid[focus_mask], 0, None)\n",
        "print(f\"🌀 Final blended valid SMAPE(0제외): {smape_ignore_zero(y_valid.values, final_valid):.6f}\")\n",
        "\n",
        "# 테스트에서 쓸 전역 파라미터 저장\n",
        "BLEND_WEIGHTS = (w1, w2, w3)\n",
        "FOCUS_ALPHA = alpha"
      ],
      "metadata": {
        "id": "38YC84HmnLLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##test"
      ],
      "metadata": {
        "id": "ROUBMjyypNxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_test_features(test_df):\n",
        "    test_df = ensure_store_menu(test_df)\n",
        "    test_df = add_calendar_features(test_df)\n",
        "    test_df = add_ts_features(test_df)\n",
        "\n",
        "    def safe_transform(le, series):\n",
        "        mapping = {k: i for i, k in enumerate(le.classes_)}\n",
        "        code = series.map(mapping).astype('Int64')  # pandas nullable int\n",
        "        unk_code = (pd.Series(mapping.values()).max() if len(mapping) else -1) + 1\n",
        "        return code.fillna(unk_code).astype(np.int32)\n",
        "\n",
        "    test_df['업장명'] = test_df['업장명'].astype(str)\n",
        "    test_df['메뉴명'] = test_df['메뉴명'].astype(str)\n",
        "    test_df['업장_code'] = safe_transform(le_store, test_df['업장명'])\n",
        "    test_df['메뉴_code'] = safe_transform(le_menu,  test_df['메뉴명'])\n",
        "    return test_df\n",
        "\n",
        "# (메뉴,요일) 평균은 train에서만 계산 → 위 grp 재사용\n",
        "def predict_test_file(path):\n",
        "    filename = os.path.basename(path)\n",
        "    prefix = re.search(r'(TEST_\\d+)', filename).group(1)\n",
        "\n",
        "    test_df = pd.read_csv(path)\n",
        "    test_df = make_test_features(test_df)\n",
        "\n",
        "    # predict_test_file 내부, feats_last 만든 직후\n",
        "    last_date = test_df.groupby('영업장명_메뉴명')['영업일자'].max()\n",
        "    last_dow  = last_date.dt.weekday  # 0~6\n",
        "\n",
        "    # 그룹별 마지막 시점 피처\n",
        "    test_df = test_df.sort_values(['영업장명_메뉴명','영업일자'])\n",
        "    last_idx = test_df.groupby('영업장명_메뉴명')['영업일자'].idxmax()\n",
        "    feats_last = test_df.loc[last_idx, ['영업장명_메뉴명','업장명','메뉴명'] + feat_cols].copy()\n",
        "    feats_last = feats_last.dropna(subset=feat_cols)\n",
        "    if len(feats_last) == 0:\n",
        "        return pd.DataFrame(columns=['영업일자','영업장명_메뉴명','매출수량'])\n",
        "\n",
        "    # 공통 X\n",
        "    def infer_one_h(h):\n",
        "        tmp = feats_last.copy()\n",
        "        tmp['h'] = h\n",
        "        Xh = tmp[feat_cols + ['h']].astype(np.float32)\n",
        "        # LGBM\n",
        "        lgbh = np.clip(lgbm.predict(Xh, num_iteration=lgbm.best_iteration_), 0, None)\n",
        "        # ZI\n",
        "        ph  = clf.predict_proba(Xh)[:,1]\n",
        "        muh = np.expm1(reg.predict(Xh))\n",
        "        zih = np.clip(ph * muh, 0, None)\n",
        "\n",
        "        # ---- DOW naive (horizon별 요일 반영) ----\n",
        "        keys = tmp['영업장명_메뉴명'].values\n",
        "        dow_h = (last_dow.loc[keys].values + h) % 7\n",
        "\n",
        "        idx = pd.MultiIndex.from_arrays([keys, dow_h])\n",
        "        naive_vals = grp_dow.reindex(idx).to_numpy()\n",
        "\n",
        "        if np.any(pd.isna(naive_vals)):\n",
        "            menu_mean = train.groupby('영업장명_메뉴명')['매출수량'].mean()\n",
        "            fill1 = pd.Series(keys).map(menu_mean).to_numpy()\n",
        "            fill1 = np.where(pd.isna(fill1), float(menu_mean.mean()), fill1)\n",
        "            naive_vals = np.where(pd.isna(naive_vals), fill1, naive_vals)\n",
        "\n",
        "        # ---- 블렌드 (검증에서 학습된 전역 가중치 + 포커스 업장 α) ----\n",
        "        w1, w2, w3 = BLEND_WEIGHTS\n",
        "        blend = w1*lgbh + w2*zih + w3*naive_vals\n",
        "\n",
        "        stores = tmp['업장명'].map(norm_name).values  # ← tmp로 통일\n",
        "        focus_mask = np.array([any(fs in st for fs in FOCUS_STORES_NORM) for st in stores])\n",
        "\n",
        "        blend[focus_mask] = FOCUS_ALPHA * blend[focus_mask]\n",
        "        blend = np.clip(blend, 0, None)\n",
        "\n",
        "\n",
        "        out = tmp[['영업장명_메뉴명']].copy()\n",
        "        out['영업일자'] = f\"{prefix}+{h}일\"\n",
        "        out['매출수량'] = blend\n",
        "        return out\n",
        "\n",
        "    outs = [infer_one_h(h) for h in HORIZONS]\n",
        "    return pd.concat(outs, ignore_index=True)\n",
        "\n",
        "all_preds = []\n",
        "for p in test_files:\n",
        "    all_preds.append(predict_test_file(p))\n",
        "pred_full = pd.concat(all_preds, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "O5zxCXyepHXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##submission"
      ],
      "metadata": {
        "id": "cjPuirBnpKtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 제출 변환\n",
        "# -----------------------------\n",
        "def convert_to_submission(pred_df, sample_df):\n",
        "    out = sample_df.copy()\n",
        "    pred_df = pred_df.copy()\n",
        "    pred_df['영업장명_메뉴명'] = pred_df['영업장명_메뉴명'].apply(normalize_key)\n",
        "    pred_df['영업일자'] = pred_df['영업일자'].astype(str)\n",
        "\n",
        "    # 샘플 컬럼 정규화\n",
        "    col_map = {}\n",
        "    for c in out.columns:\n",
        "        if c == '영업일자':\n",
        "            continue\n",
        "        col_map[c] = normalize_key(c)\n",
        "\n",
        "    # key → value\n",
        "    pred_df['norm_menu'] = pred_df['영업장명_메뉴명'].apply(normalize_key)\n",
        "    key = list(zip(pred_df['영업일자'], pred_df['norm_menu']))\n",
        "    pred_dict = dict(zip(key, pred_df['매출수량']))\n",
        "\n",
        "    # 채우기\n",
        "    for i in range(len(out)):\n",
        "        date = str(out.loc[i, '영업일자'])\n",
        "        for c in out.columns[1:]:\n",
        "            out.loc[i, c] = pred_dict.get((date, col_map[c]), 0)\n",
        "    return out\n",
        "\n",
        "submission = convert_to_submission(pred_full, sample_submission)\n",
        "submission.to_csv(SAVE_PATH, index=False, encoding=\"utf-8-sig\")\n",
        "print(\"최종 제출 파일 저장 완료:\", SAVE_PATH)\n",
        "print(submission.head(3).to_string()[:1000])"
      ],
      "metadata": {
        "id": "Z4t5dMT6pKF-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}